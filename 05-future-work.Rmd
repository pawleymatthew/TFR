# Future work {#future-work}
\chaptermark{Future work}
\minitoc

\noindent This concluding chapter will identify limitations in the existing methodology and outline some ideas for how these can be addressed in future work.

## Choosing $m$ in applications {#choosing-m-in-applications}

The parameter $m$ plays a crucial role in determining the realism and usefulness of the synthetic events generated by the sampling algorithm of @rohrbeckSimulatingFloodEvent2021. However, a good criterion for selecting $m$ has not been established thus far. The standard approach is to perform a spectral analysis and choose $m$ such that $\sum_{i=1}^m \lambda_i/\sum_{i=1}^d \lambda_i$ is acceptably close to 1. We seek more sophisticated approaches. The key question is: what is the best way to measure the agreement between two datasets of extreme events? Future work will aim to formulate a suitable notion of error upon which to base a selection criterion for $m$.

An important question to address is whether we should be considering reconstruction error or simulation error. The analysis in Section \@ref(fr-tuning-m) suggests these notions of error do not coincide; this should be confirmed through theoretical work and further numerical studies. Within each of these categories, there are several notions of error. For example, we could consider quantities such as:

1. The event reconstruction error, $\sum_{t=1}^T \snorm{\hat{\bm{x}}_t(m)-\tilde{\bm{x}}_t}$, where $\hat{\bm{x}}_t(m)$ is the $m$-eigenvector reconstruction of the observed event $\tilde{\bm{x}}_t$;
2. The empirical conditional risk, see \@ref(eq:empirical-risk);
3. The TPDM error, $\snorm{\hat{\Sigma}(m)-\hat{\Sigma}}$, where $\hat{\Sigma}(m)$ is the TPDM estimated from a set of reconstructed/simulated events;
4. The $\chi$ error, $\snorm{\hat{\chi}(m)-\hat{\chi}}$, where $\hat{\chi}(m)$ is the matrix of tail correlation coefficients estimated from a set of reconstructed/simulated events;

The first measure is unlikely to be useful, because it will be distorted by the inaccurate magnitudes of the reconstructed events, whereas our focus is on capturing the dependence structure. The tail correlation measure $\chi$ is more robust than the TPDM in terms of its sensitivity to very extreme events in the dataset, so the fourth measure may be superior to the third. Some of these measures may be susceptible to rewarding overfitted models. A simulated event set that closely matches the observed event set will achieve a low error but is not useful in practice. 

## TPDM estimators

Recall from Section \@ref(estimation-tpdm) that there are two different TPDM estimators being used in the literature. The vector-based estimator is a much more natural estimator, in the sense that it satisfies the theoretical properties of the TPDM, but it may perform poorly if extremal behaviour is highly localised. This trade-off is somewhat unsatisfactory and hints at a broader underlying issue. Therefore, we propose a critical comparison of the TPDM estimators and exploring ways they can be improved. 

### Bias correction

A first step for improving the TPDM estimator is to reduce bias. Threshold-based estimators are known to overestimate dependence between weakly dependent variables [@huserLikelihoodEstimatorsMultivariate2016]. @fixSimultaneousAutoregressiveModels2021 address this by imposing that extremal dependence is close to zero at large distances. This assumption is reasonable in some applications (e.g.\ extreme precipitation on a large spatial domain) but not in others. For example, in an analysis of extreme river flow, two flow-connected sites may exhibit extremal dependence even if they are far apart, so it is more appropriate to correct bias based on the graph structure of the river network. However, this is counter to the spirit of our methods, which are supposed to be data-driven without the need to incorporate prior domain knowledge. Here we may turn to the wider extremes literature. The techniques described in @ledfordStatisticsIndependenceMultivariate1996 might provide a useful starting point. They use so-called censored estimators to deal with the issues encountered when estimating the dependence between variables that are near independence. On the basis of results obtained via simulation studies, @huserLikelihoodEstimatorsMultivariate2016 also recommend censored estimators, as well as choosing higher thresholds. 

### Permitting asymptotic independence 

A similar, but separate, issue is to generalise extremal PCA so that it doesn't assume asymptotic dependence. Asymptotic independence is a degenerate case within the multivariate regular variation framework that underlies current methods, which cannot be attained from finite samples. We now outline some possible ways this could be achieved. Future work will involve testing and comparing these methods and/or devising some other new methodology.

An initial idea is to determine whether extremal PCA could be adapted to work with an alternative notion of regular variation called sparse regular variation [@meyerSparseRegularVariation2021]. Sparse regular variation redefines the angular component from $\bm{X}/\snorm{\bm{X}}$ to $\pi(\bm{X}/t)$, where $t$ is the radial threshold and $\pi$ is the Euclidean projection onto the unit simplex. Unlike with the self-normalised vector, the projected points need not lie in the interior of the simplex. Therefore, sparse regular variation better captures the sparse structure of $H$ and permits asymptotic independence. Unfortunately, the independence between the radial and angular components is lost (though the dependence relation is known), so adapting extremal PCA for this framework may be complicated. 

A second approach to consider is employ clustering before estimating the TPDM. Specifically, we partition the components of $\bm{X}$ into clusters, estimate the TPDM within each cluster separately, and then combine the cluster-based TPDMs to form the overall TPDM estimate. The inter-cluster entries of $\hat{\Sigma}$ are set equal to zero, meaning that components in different clusters are assumed to be asymptotically independent. Remark 3.3 in @fomichovDetectionGroupsConcomitant2020, which alludes to links between spherical clustering and extremal PCA, may provide useful insight into the underlying theory of this idea. Figure \@ref(fig:fr-tpdm-clusters-levelplot) illustrates the result when this process is applied to the French rainfall data. The left plot shows the standard vector-based TPDM estimate we computed earlier, for reference. The right-hand TPDM is computed after performing clustering using the methodology of @bernardClusteringMaximaSpatial2013, i.e.\ the PAM algorithm with F-madogram distances and $K=5$. Introducing this additional step means there are now two hyperparameters ($K$ and $m$) to be tuned. 

```{r cache = TRUE}
# estimate TPDM in standard way
SigmavHR <- estTPDM(Xtilde, method = "v", u = 0.85)
# estimate TPDM of each cluster separately, then glue them together
clusterK <- PAMcluster(X = Xtilde, K = 5)
SigmavHRcluster <- matrix(0, nrow = 92, ncol = 92)
for (l in 1:5){
  ids <- which(clusterK$clustering == l)
  clusterTPDM <- estTPDM(Xtilde[,ids], method = "v", u = 0.85)
  for (i in seq_along(ids)){
    for (j in seq_along(ids)){
      SigmavHRcluster[ids[i],ids[j]] <- clusterTPDM[i,j]
    }
  }
}
```

```{r fr-tpdm-clusters-levelplot, fig.align='center', fig.cap="Left: the standard vector-based TPDM estimate for the French rainfall data. Right: the cluster vector-based TPDM estimate obtained after performing PAM clustering with F-madogram distances and $K=5$.", fig.path='figures/', fig.scap="Example of a TPDM estimate obtained after clustering.", fig.show="hold", dev='pdf', out.width="48%"}
levelplotmatrix(SigmavHR, xlab = "i", ylab = "j")
levelplotmatrix(SigmavHRcluster, xlab = "i", ylab = "j")
``` 

A third possible approach is very similar to the previous method, except the clustering step is replaced with another method for detecting the dependence structure. For example, there are a range of existing methods for detecting the faces of the unit sphere charged with $H$-mass [@goixSparseRepresentationMultivariate2017; @simpsonDeterminingDependenceStructure2019; @meyerSparseRegularVariation2021]; the TPDM can then be estimated separately on each face.

## Performing extremal PCA with large $m$

In their study of extreme rainfall in the UK, @rohrbeckSimulatingFloodEvent2021 found that six extremal principal components were sufficient to describe large-scale extremal behaviour. In other applications, such as the French rainfall data, a larger number may be required. In such cases where drastic dimension reduction is not feasible, we may be need to perform extremal PCA with moderate or large $m$. 

One approach is to explore whether there is any structure in the extremal principal components that can be exploited. We know that the extremal principal components are not asymptotically independent, but they do satisfy a property that implies balance between quadrants [@cooleyDecompositionsDependenceHighdimensional2019, Section 5]. It is worth investigating whether this or some other structure can be exploited. As a starting point, we could try applying one of the clustering or face detection techniques to identify groups of extremal principal components with strong dependence, and then fit submodels to each of these groups. 

Another approach is to explore alternatives to kernel density estimation (KDE) when fitting the flexible model for the leading components of $H_V$. KDE performance tends to worsen in high dimensions [@wangNonparametricDensityEstimation2019], so we will consult the literature to find suitable alternative models for spherical data. The semi-parametric mixture models (which become fully-parametric when the number of mixture components is fixed) are potential candidates.

## Uncertainty quantification

At present, the methodology does not include any considerations for uncertainty quantification. Uncertainty arises at several occasions in the modelling process, such as the estimation of the marginal distributions, the estimation of the TPDM, and fitting the KDE for $\bm{Z}$. Initially, we will focus on quantifying uncertainty in the TPDM and studying how it propagates through to the simulations. A natural way to estimate uncertainty is via a non-parametric bootstrap procedure. Suppose we are given a set of $n$ observations of a random vector $\bm{X}$. For simplicity, we would initially consider a case where the marginal transformation has already been performed, and the true distribution of $\bm{X}$ is known, e.g.\ $d$-variate max-stable Hüsler-Reiss. We then sample with replacement from the original set of observations, estimate the TPDM, and generate a set of samples of $\bm{X}$. Then one can study the sampling distribution of the TPDM estimates and estimate the bias and variance. Moreover, if we have a way of estimating the optimal $m$ (Section \@ref(choosing-m-in-applications)), we can also study variation in the estimates $\hat{m}$ - @fixSimultaneousAutoregressiveModels2021 employ this approach to derive confidence intervals for the SAR parameter $\rho$. The procedure of computing a large number of bootstrapped TPDMs may lead to better approximations for the distribution of $\bm{X}$ (this can be checked in a simulation setting) - if this is the case, it opens up another route for advancing the methodology. 
